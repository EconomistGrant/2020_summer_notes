---
title: "15.455: Model Testing and Identification"
subtitle: "Testing the Random Walk"
author: "pfm"
date: "July 22, 2020"
output: 
  html_notebook:
  df_print: paged
  toc: yes
---

*Notebooks in R Markdown*:
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing a code chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

# Preliminaries

Before we get started, let's install a few **packages**.

- The `install.packages` command is run **once** to download the software to your computer.
- The `library` command is run **one time per session** in order to load a packages functions and make them available.

Here are packages we'll use today.  If you need to install them, uncomment the lines in the block below and run it one time.

```{r prelim}
#install.packages("tidyverse")                # To install a single package
#install.packages(c("vrtest","ggplot2"))      # Additional packages for Variance Ratios and for plotting
```


Now we'll load the parts we need.
```{r loading}
library(readr)
library(ggplot2)
library(vrtest)
```

# Tootsie Roll

Let's load some data and look at default summary stats for data from Tootsie Roll (TR).  The file tr.txt contains daily prices, adjusted for splits and dividends, from Yahoo Finance.  (Tootsie Roll actually has an unusual stock dividend history.  By using the Adjusted Close data, we don't need to worry about those details for now.)

**Technical note:** Data is often exchanged using "flat files," which are plain text files that can be read using a simple text editor.  Although they are usually structured, format details can differ; so it's essential to pay attention to elements that might vary from one source to another such as

- Column headers (yes/no, naming conventions)
- Line breaks (CR, LF, CR/LF)
- Delimiters
- Escape characters
- Ordering
- Data types (integer/float, text/numeric)
- Date formatting (YMD, MDY, %Y/%y, etc.)
- Data errors!

Advice: don't take anything for granted.  Check your data!

```{r input data}
# Read in data file -- set your own local directory for file location

filename2020 <- "http://obelix.mit.edu/data/tr2020.csv"

tr2020 <- read_csv(filename2020)


filename <- "http://obelix.mit.edu/data/tr.txt"

tr <- read_delim(filename, "\t",
  escape_double = FALSE, col_types = cols(Date = col_date(format = "%m/%d/%Y")),
  trim_ws = TRUE)


```

Here is what the price and daily returns look like over time

```{r price charts}
plot(tr$Date,tr$`Adj Close`,type="l",xlab="Time",ylab="Price",main="TR Adjusted Price 1988-2017");grid()

plot(tr$Date,tr$Return,type="l",xlab="Time",ylab="Price",main="TR Daily Returns 1988-2017");grid()
```

The daily return series is noisy, and the mean value is barely visible.  However the scale of the noise fluctuations is not constant over time.  This is the phenomenon of time-varying volatility.  Compare the graph above with the simulation below, in which simulated returns have the same average volatility and zero mean.

```{r white noise}
plot(tr$Date,rnorm(nrow(tr))*sd(tr$Return),type="l",ylim=c(-0.18,0.18),xlab="Time",ylab="Price",main="White Noise Process with TR Volatility");grid()
```

## Summary statistics and return distribution

These are high-level summary stats that R provides for any data frame.

```{r summary stats}
summary(tr)
```

Now we extract the time series of prices and compute the series of 1-day returns 
$$r_t = \log(P_t/P_{t-1}) = \log(P_t/P_0) - \log(P_{t-1}/P_0)$$ .

```{r prices defined}
P <- tr$`Adj Close` # Define price using split- & dividend-adjusted values
N <- length(P); N
```


```{r log returns defined}
r <- diff(log(P)) #Define log returns from successive daily prices
```

# Annualization conventions

We typically report return and risk measures in annualized terms.  By convention, we assume a typical year has 252 trading days and use the following rules:

- Annualized return = 252 * (Daily return)
- Annualized std. dev. = sqrt(252) * (Daily std. dev)

For monthly returns, replace the 252 by 12.


```{r annualization}
 
mean(r)*252     # Mean return for TR (annualize by 252 days/year)
sd(r)*sqrt(252) # Volatility of TR (annualize with square root!)

summary(r)
```

The histogram of returns has fat tails (and therefore a thin middle).  Because it is the unconditional distribution of returns, independent of their time ordering, it tells nothing about the causal structure of return correlations.


```{r return distribution}
hist(r, breaks=50)
```

# Lo & MacKinlay

Following Lo & MacKinlay, we ask whether the measured sample variance of returns grows linearly as function of the observation interval.


```{r variance plot}
Variance <- var(diff(log(P))) 

for (n in 2:100) {
  Variance[n] <- var(diff(log(P[seq(from=n, to=N, by=n)])))
}

plot(Variance,xlab="n",main="Variance of Returns From n-day Observations");grid()

```

Looks linear, doesn't it?  Is that good enough?  What about the slope?  What about the raggedness on the right side of the graph?  Is that simply noise due to have a smaller number of samples when the window size gets large?  Or could there be a systematic deviation of the linear rule hiding in the graph?

# Variance and Ratios

Here we define functions for $\widehat \sigma^2_c$, which is a function of a series of observations $X_t$ and an aggregation length $q$.

The $z$-statistics and $p$-values follow from the distribution of the estimator as a random variable

```{r variance metrics defined}
variance.c <- function(X, q) {
# Compute variance statistic from overlapping q-period windows
# See Lo & MacKinlay (1988), p. 47, Eq. 12 
  
  T     <- length(X) - 1  
  mu    <- (X[T+1] - X[1])/T  
  m     <- (T-q)*(T-q+1)*q/T
  sumsq <- 0  
  for (t in q:T) { 
    sumsq <- sumsq + (X[t+1] - X[t-q+1] - q*mu)^2 
  }  
  return(sumsq/m)
}

z <- function(X, q) {
# Compute sampling statistic for variance ratio
# See Lo & MacKinlay (1988), p. 47, last line (after Eqs. 12-14)  
  T <- length(X) - 1  
  c <- sqrt(T*(3*q)/(2*(2*q-1)*(q-1)))  
  M <- variance.c(X,q)/variance.c(X,1) - 1  
  z <- c*M
  return(z)
}

Vc      <- 0; for (q in 1:100) {Vc[q] <- variance.c(log(P),q)}
zstats  <- 0; for (q in 2:100) {zstats[q] <- z(log(P),q) }
pValues <- 2*pnorm(-abs(zstats))
barplot(zstats, ylab="z",xlab="q",main="z Statistics of Variance Ratio Test")

```

## Interpreting the test statistics

The test statistic $z(q)$ was constructed to be normally distributed as ${\cal N}(0,1)$ if the data followed a random walk and scaled accordingly.  From the graph, we see that all of these $z$-statistics are greater than two in magnitude -- and they all have the same sign.  They are not consistent with the first random walk hypothesis, and their systematic deviation suggests that the model needs to be extended rather than discarded by including serial correlation.



```{r scaled volatility}
sigma <- sqrt(252)*sd(diff(log(P))) 

for (n in 2:100) {
  sigma[n] <- sqrt(252/n)*sd(diff(log(P[seq(from=n, to=N, by=n)])))
}

barplot(sigma,xlab="n",ylab="Standard Deviation (annualized) / sqrt(n)",main="Volatility Scaling of Returns From n-day Observations (TR)");grid()

```

```{r scaled volatility simulation}
P.MC <- exp(cumsum(rnorm(N)*0.02)) # Monte Carlo returns 32% vol
sigma.MC <- sqrt(252)*sd(diff(log(P.MC))) 

for (n in 2:100) {
  sigma.MC[n] <- sqrt(252/n)*sd(diff(log(P.MC[seq(from=n, to=N, by=n)])))
}

barplot(sigma.MC,xlab="n",ylab="Standard Deviation (annualized) / sqrt(n)",main="Volatility Scaling of Returns From n-day Observations (Sim)");grid()

```




# Order determination:  AR(2) Example


```{r}
a_0   <-  0.001 
a_1   <- -0.1
a_2   <-  0.4
sigma <- 1
Nt    <-  1000; 
r     <-  matrix(0,Nt,1)
z     <- matrix(rnorm(Nt), ncol=1)

for (t in 3:Nt)  {
  r[t] <- a_0 + a_1*r[t-1] + a_2*r[t-2] + sigma*z[t]
}

plot(cumsum(r),type="l",main="AR(2) Sample Path",xlab="Time",ylab="r");grid()

acf(r, main="AR(2) Sample Autocorrelation Function")
pacf(r, main="AR(2) Sample Partial Autocorrelation Function")

```
# Model estimation: AR(2) example

AR models can be viewed and estimated as multivariate linear regression models, where the right-hand size "independent" variables are just the lagged observation series.  For the process to be consistent, the regression residuals must be (approximately) an independent, uncorrelated white noise process.

In R, there are functions that do the same thing.  The most general case, for ARIMA models, includes AR(p) as a special case of ARIMA(p,0,0)

```{r}
# Method (1) Numerical estimation using ordinary least squares
y   <- r[3:Nt]
x1  <- r[2:(Nt-1)]
x2  <- r[1:(Nt-2)]

AR2model <- lm(y ~ x1 + x2)
summary(AR2model)

# Method (2): Using the arima function
arima(r, order=c(2,0,0))
```

What if we get the order incorrect?  If the data is not too noisy, the redundant terms will have coefficients close to zero.

```{r}
arima(r, order=c(5,0,0))
```



# Order determination:  MA(2) Example


```{r}
mu  <-  0.0
c_0 <-  1.0; 
c_1 <- -0.1; 
c_2 <-  0.4; 
Nt  <-  1000; 
r   <-  matrix(0,Nt,1)
z <- matrix(rnorm(Nt), ncol=1)

# Generate return series recursively.  Assign z=0 before initial observation to bootstrap.
r[1] <- mu + c_0*z[1]
r[2] <- mu + c_0*z[2] + c_1*z[1]
for (t in 3:Nt)  {
  r[t] <- mu + c_0*z[t] + c_1*z[t-1] + c_2*z[t-2]
}

plot(cumsum(r),type="l",main="MA(2) Sample Path",xlab="Time",ylab="r");grid()

acf(r, main="MA(2) Sample Autocorrelation Function")
pacf(r, main="MA(2) Sample Partial Autocorrelation Function")

```
# Model estimation:  MA(2)

```{r}
arima(r,order = c(0,0,2))
```


